 * Serving Flask app "fedavgclient" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.1.2:9501/ (Press CTRL+C to quit)
 * Restarting with inotify reloader
 * Debugger is active!
 * Debugger PIN: 658-733-059
{'response': 'ok'}
127.0.0.1 - - [24/Nov/2022 23:23:04] "[37mGET /start HTTP/1.1[0m" 200 -
{'response': 'ok'}
2022-11-24 23:23:04.650756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/hamid/Documents/indy-sdk/libindy/target/debug
2022-11-24 23:23:04.650830: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Model: FedAvg, Round: 1/3, Client 2/5, Dataset Size: 12000
  1/675 [..............................] - ETA: 5:11 - loss: 2.2310 - accuracy: 0.1250 18/675 [..............................] - ETA: 1s - loss: 1.7749 - accuracy: 0.4271   34/675 [>.............................] - ETA: 2s - loss: 1.4032 - accuracy: 0.5790 51/675 [=>............................] - ETA: 1s - loss: 1.1700 - accuracy: 0.6556 67/675 [=>............................] - ETA: 1s - loss: 1.0254 - accuracy: 0.6987 81/675 [==>...........................] - ETA: 1s - loss: 0.9573 - accuracy: 0.7145 96/675 [===>..........................] - ETA: 1s - loss: 0.8946 - accuracy: 0.7344112/675 [===>..........................] - ETA: 1s - loss: 0.8306 - accuracy: 0.7533128/675 [====>.........................] - ETA: 1s - loss: 0.7757 - accuracy: 0.7690145/675 [=====>........................] - ETA: 1s - loss: 0.7337 - accuracy: 0.7802161/675 [======>.......................] - ETA: 1s - loss: 0.7075 - accuracy: 0.7877178/675 [======>.......................] - ETA: 1s - loss: 0.6779 - accuracy: 0.7974195/675 [=======>......................] - ETA: 1s - loss: 0.6553 - accuracy: 0.8010212/675 [========>.....................] - ETA: 1s - loss: 0.6434 - accuracy: 0.8051220/675 [========>.....................] - ETA: 1s - loss: 0.6336 - accuracy: 0.8074238/675 [=========>....................] - ETA: 1s - loss: 0.6145 - accuracy: 0.8125256/675 [==========>...................] - ETA: 1s - loss: 0.5942 - accuracy: 0.8191273/675 [===========>..................] - ETA: 1s - loss: 0.5757 - accuracy: 0.8253290/675 [===========>..................] - ETA: 1s - loss: 0.5673 - accuracy: 0.8274308/675 [============>.................] - ETA: 1s - loss: 0.5537 - accuracy: 0.8324325/675 [=============>................] - ETA: 1s - loss: 0.5414 - accuracy: 0.8358341/675 [==============>...............] - ETA: 1s - loss: 0.5311 - accuracy: 0.8385358/675 [==============>...............] - ETA: 1s - loss: 0.5158 - accuracy: 0.8436375/675 [===============>..............] - ETA: 0s - loss: 0.5039 - accuracy: 0.8470391/675 [================>.............] - ETA: 0s - loss: 0.4927 - accuracy: 0.8505408/675 [=================>............] - ETA: 0s - loss: 0.4851 - accuracy: 0.8528425/675 [=================>............] - ETA: 0s - loss: 0.4760 - accuracy: 0.8557442/675 [==================>...........] - ETA: 0s - loss: 0.4689 - accuracy: 0.8580458/675 [===================>..........] - ETA: 0s - loss: 0.4631 - accuracy: 0.8597475/675 [====================>.........] - ETA: 0s - loss: 0.4555 - accuracy: 0.8617492/675 [====================>.........] - ETA: 0s - loss: 0.4471 - accuracy: 0.8642509/675 [=====================>........] - ETA: 0s - loss: 0.4402 - accuracy: 0.8663527/675 [======================>.......] - ETA: 0s - loss: 0.4317 - accuracy: 0.8690543/675 [=======================>......] - ETA: 0s - loss: 0.4247 - accuracy: 0.8707551/675 [=======================>......] - ETA: 0s - loss: 0.4216 - accuracy: 0.8717568/675 [========================>.....] - ETA: 0s - loss: 0.4145 - accuracy: 0.8740585/675 [=========================>....] - ETA: 0s - loss: 0.4092 - accuracy: 0.8754602/675 [=========================>....] - ETA: 0s - loss: 0.4045 - accuracy: 0.8766620/675 [==========================>...] - ETA: 0s - loss: 0.4010 - accuracy: 0.8777637/675 [===========================>..] - ETA: 0s - loss: 0.3976 - accuracy: 0.8784654/675 [============================>.] - ETA: 0s - loss: 0.3937 - accuracy: 0.8800673/675 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8817675/675 [==============================] - 3s 4ms/step - loss: 0.3867 - accuracy: 0.8819 - val_loss: 0.2622 - val_accuracy: 0.9242
/home/hamid/Documents/FedShare-Anonymous/fedavgclient.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  round_weight = np.array(model.get_weights())
/home/hamid/Documents/FedShare-Anonymous/fedavgclient.py:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  pickle_model = pickle.dumps(np.array(layers))
{'response': 'ok'}
{'response': 'ok'}
Sent to fedavg server.
[Upload] Size of the object to send to server is 2342887
Sent 0 to server
[DOWNLOAD] Total download cost so far: 0
[UPLOAD] Total upload cost so far: 2342887
********************** Round 1 completed **********************
Waiting to receive response from server...
{'response': 'ok'}
