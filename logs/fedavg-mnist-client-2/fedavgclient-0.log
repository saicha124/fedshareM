 * Serving Flask app "fedavgclient" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.1.1:9500/ (Press CTRL+C to quit)
 * Restarting with inotify reloader
 * Debugger is active!
 * Debugger PIN: 106-656-382
{'response': 'ok'}
127.0.0.1 - - [24/Nov/2022 23:23:04] "[37mGET /start HTTP/1.1[0m" 200 -
{'response': 'ok'}
2022-11-24 23:23:04.659947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/hamid/Documents/indy-sdk/libindy/target/debug
2022-11-24 23:23:04.659995: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Model: FedAvg, Round: 1/3, Client 1/5, Dataset Size: 12000
  1/675 [..............................] - ETA: 5:01 - loss: 2.2719 - accuracy: 0.2500 19/675 [..............................] - ETA: 1s - loss: 1.8536 - accuracy: 0.4145   35/675 [>.............................] - ETA: 1s - loss: 1.5384 - accuracy: 0.5357 52/675 [=>............................] - ETA: 1s - loss: 1.2901 - accuracy: 0.6178 68/675 [==>...........................] - ETA: 1s - loss: 1.1401 - accuracy: 0.6664 84/675 [==>...........................] - ETA: 1s - loss: 1.0418 - accuracy: 0.6964101/675 [===>..........................] - ETA: 1s - loss: 0.9744 - accuracy: 0.7153118/675 [====>.........................] - ETA: 1s - loss: 0.9025 - accuracy: 0.7378135/675 [=====>........................] - ETA: 1s - loss: 0.8444 - accuracy: 0.7528152/675 [=====>........................] - ETA: 1s - loss: 0.7965 - accuracy: 0.7660170/675 [======>.......................] - ETA: 1s - loss: 0.7469 - accuracy: 0.7801187/675 [=======>......................] - ETA: 1s - loss: 0.7171 - accuracy: 0.7881204/675 [========>.....................] - ETA: 1s - loss: 0.6812 - accuracy: 0.7996220/675 [========>.....................] - ETA: 1s - loss: 0.6571 - accuracy: 0.8068235/675 [=========>....................] - ETA: 1s - loss: 0.6415 - accuracy: 0.8106251/675 [==========>...................] - ETA: 1s - loss: 0.6306 - accuracy: 0.8137268/675 [==========>...................] - ETA: 1s - loss: 0.6129 - accuracy: 0.8174285/675 [===========>..................] - ETA: 1s - loss: 0.5911 - accuracy: 0.8235302/675 [============>.................] - ETA: 1s - loss: 0.5742 - accuracy: 0.8284320/675 [=============>................] - ETA: 1s - loss: 0.5566 - accuracy: 0.8340337/675 [=============>................] - ETA: 1s - loss: 0.5495 - accuracy: 0.8357354/675 [==============>...............] - ETA: 1s - loss: 0.5398 - accuracy: 0.8385372/675 [===============>..............] - ETA: 0s - loss: 0.5272 - accuracy: 0.8422390/675 [================>.............] - ETA: 0s - loss: 0.5176 - accuracy: 0.8457405/675 [=================>............] - ETA: 0s - loss: 0.5106 - accuracy: 0.8475421/675 [=================>............] - ETA: 0s - loss: 0.5046 - accuracy: 0.8489438/675 [==================>...........] - ETA: 0s - loss: 0.4992 - accuracy: 0.8505456/675 [===================>..........] - ETA: 0s - loss: 0.4884 - accuracy: 0.8538473/675 [====================>.........] - ETA: 0s - loss: 0.4835 - accuracy: 0.8549491/675 [====================>.........] - ETA: 0s - loss: 0.4736 - accuracy: 0.8574508/675 [=====================>........] - ETA: 0s - loss: 0.4651 - accuracy: 0.8602525/675 [======================>.......] - ETA: 0s - loss: 0.4579 - accuracy: 0.8620543/675 [=======================>......] - ETA: 0s - loss: 0.4503 - accuracy: 0.8645560/675 [=======================>......] - ETA: 0s - loss: 0.4438 - accuracy: 0.8667568/675 [========================>.....] - ETA: 0s - loss: 0.4415 - accuracy: 0.8676585/675 [=========================>....] - ETA: 0s - loss: 0.4360 - accuracy: 0.8699602/675 [=========================>....] - ETA: 0s - loss: 0.4283 - accuracy: 0.8723619/675 [==========================>...] - ETA: 0s - loss: 0.4221 - accuracy: 0.8742635/675 [===========================>..] - ETA: 0s - loss: 0.4165 - accuracy: 0.8756652/675 [===========================>..] - ETA: 0s - loss: 0.4146 - accuracy: 0.8758669/675 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.8771675/675 [==============================] - 3s 4ms/step - loss: 0.4090 - accuracy: 0.8773 - val_loss: 0.2596 - val_accuracy: 0.9175
/home/hamid/Documents/FedShare-Anonymous/fedavgclient.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  round_weight = np.array(model.get_weights())
/home/hamid/Documents/FedShare-Anonymous/fedavgclient.py:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  pickle_model = pickle.dumps(np.array(layers))
{'response': 'ok'}
{'response': 'ok'}
Sent to fedavg server.
[Upload] Size of the object to send to server is 2342887
Sent 0 to server
[DOWNLOAD] Total download cost so far: 0
[UPLOAD] Total upload cost so far: 2342887
********************** Round 1 completed **********************
Waiting to receive response from server...
{'response': 'ok'}
